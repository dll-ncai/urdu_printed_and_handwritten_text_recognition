{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2 \n",
    "from tacobox import Taco\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_upti = pd.read_csv(\"data_upti/train.csv\")\n",
    "# train_df_ticker = pd.read_csv(\"data_ticker/train.csv\")\n",
    "# test_df_upti = pd.read_csv(\"data_upti/test.csv\")\n",
    "# test_df_ticker = pd.read_csv(\"data_ticker/test.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"data_ihtr/trainingset/urdu/data.csv\")\n",
    "# eval_df = pd.read_csv(\"data_ihtr/validationset/urdu/data.csv\")\n",
    "\n",
    "\n",
    "# train_df_icdar = pd.read_csv(\"data_upti/train_icdar.csv\")\n",
    "train_df = pd.read_csv(\"data_uhwr/train.csv\")\n",
    "# train_df = pd.read_csv\n",
    "# eval_df = pd.read_csv(\"data_upti/test_icdar.csv\")\n",
    "eval_df = pd.read_csv(\"data_uhwr/val.csv\")\n",
    "test_df = pd.read_csv(\"data_uhwr/test.csv\")\n",
    "# books = pd.read_csv(\"./data_urti/books.csv\")\n",
    "# magazines = pd.read_csv(\"./data_urti/magazines.csv\")\n",
    "# novels = pd.read_csv(\"./data_urti/novels.csv\")\n",
    "# poetry = pd.read_csv(\"./data_urti/poetry.csv\")\n",
    "\n",
    "# df = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_uhwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1 = pd.read_csv(\"MMU-OCR-21/TextLines/UrduTextLineNastaleeqOutput.csv\")[['ImagePath', 'UrduLines']].rename(columns={\"ImagePath\": \"file_name\", \"UrduLines\":\"text\"})\n",
    "# df_2 = pd.read_csv(\"MMU-OCR-21/TextLines/UrduTextLinesNaskhOutput.csv\")[['ImagePath', 'UrduLines']].rename(columns={\"ImagePath\": \"file_name\", \"UrduLines\":\"text\"})\n",
    "# df_3 = pd.read_csv(\"MMU-OCR-21/TextLines/UrduTextLinesTehreerOutput.csv\")[['ImagePath', 'UrduLines']].rename(columns={\"ImagePath\": \"file_name\", \"UrduLines\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_icdar = train_df_icdar.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.concat([train_df_icdar[:len(train_df_uhwr)], train_df_uhwr]).reset_index(drop=True).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"data_uhwr/train.csv\")\n",
    "# eval_df = pd.read_csv(\"data_uhwr/val.csv\")\n",
    "# test_df = pd.reaad_csv('data_uhwr/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.concat([train_df_upti, train_df_ticker]).reset_index().drop(\"index\", axis=1)\n",
    "# test_df = pd.concat([test_df_upti, test_df_ticker]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['text'].to_csv(\"corpus.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class HWRDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_width=1600, \n",
    "                 input_height=64,\n",
    "                 aug = False,\n",
    "                 taco_aug_frac=0.9):\n",
    "        self.df = df\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mytaco = Taco(\n",
    "            cp_vertical=0.2,\n",
    "            cp_horizontal=0.25,\n",
    "            max_tw_vertical=100,\n",
    "            min_tw_vertical=10,\n",
    "            max_tw_horizontal=50,\n",
    "            min_tw_horizontal=10\n",
    "        )\n",
    "        self.aug=aug\n",
    "        self.taco_aug_frac=taco_aug_frac\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        \n",
    "        image = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        pixel_values = self.preprocess(image, self.aug)\n",
    "\n",
    "        # image = cv2.resize(image, (512, 64), cv2.INTER_AREA)\n",
    "\n",
    "        # pixel_values = Image.fromarray(image.T)\n",
    "        try:\n",
    "            labels = self.tokenizer(text).input_ids\n",
    "        except:\n",
    "            labels = None\n",
    "            encoding = (None, None)\n",
    "        else:\n",
    "            labels = [self.tokenizer.bos_token_id] + [label if label != self.tokenizer.pad_token_id else -100 for label in labels]+[self.tokenizer.eos_token_id]\n",
    "            encoding = (torch.tensor(pixel_values[None,:,:]).float(), torch.tensor(labels))\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "    def preprocess(self, img, augment=True):\n",
    "        if augment:\n",
    "            img = self.apply_taco_augmentations(img)\n",
    "            \n",
    "        # scaling image [0, 1]\n",
    "        img = img/255\n",
    "        img = img.swapaxes(-2,-1)[...,::-1]\n",
    "        # img = img.swapaxes(-2,-1)\n",
    "        target = np.ones((self.input_width, self.input_height))\n",
    "        # target = np.zeros((self.input_width, self.input_height))\n",
    "        new_x = self.input_width/img.shape[0]\n",
    "        new_y = self.input_height/img.shape[1]\n",
    "        min_xy = min(new_x, new_y)\n",
    "        new_x = int(img.shape[0]*min_xy)\n",
    "        new_y = int(img.shape[1]*min_xy)\n",
    "        img2 = cv2.resize(img, (new_y,new_x))\n",
    "        target[:new_x,:new_y] = img2\n",
    "        return 1 - (target)\n",
    "        # return (target)\n",
    "        #89 1661\n",
    "\n",
    "    def apply_taco_augmentations(self, input_img):\n",
    "        random_value = random.random()\n",
    "        if random_value <= self.taco_aug_frac:\n",
    "            augmented_img = self.mytaco.apply_vertical_taco(\n",
    "                input_img, \n",
    "                corruption_type='random'\n",
    "            )\n",
    "        else:\n",
    "            augmented_img = input_img\n",
    "        return augmented_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# # initialize\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# # and train\n",
    "# tokenizer.train(files=\"corpus.txt\", vocab_size=256, min_frequency=2,\n",
    "#                 special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# tokenizer.save_model(\"vocabs/ved/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, ViTFeatureExtractor, BertTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(\"vocabs/ved/\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"vocabs/ved/\")\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.unk_token = '<unk>'\n",
    "\n",
    "\n",
    "train_dataset = HWRDataset(df=train_df,\n",
    "                           tokenizer=tokenizer,\n",
    "                           aug=True\n",
    "                           )\n",
    "eval_dataset = HWRDataset(df=eval_df,\n",
    "                         tokenizer=tokenizer,)\n",
    "\n",
    "# test_dataset = HWRDataset(df=test_df,\n",
    "#                           tokenizer=tokenizer)\n",
    "\n",
    "# books = HWRDataset(df=books,\n",
    "#                    tokenizer=tokenizer,)\n",
    "                         \n",
    "# magazines = HWRDataset(df=magazines,\n",
    "#                        tokenizer=tokenizer,)\n",
    "\n",
    "# novels = HWRDataset(df=novels,\n",
    "#                     tokenizer=tokenizer,)\n",
    "\n",
    "# poetry = HWRDataset(df=poetry,\n",
    "#                     tokenizer=tokenizer,)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))\n",
    "# print(\"Number of books examples:\", len(books))\n",
    "# print(\"Number of magazines examples:\", len(magazines))\n",
    "# print(\"Number of novels examples:\", len(novels))\n",
    "# print(\"Number of poetry examples:\", len(poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "# encoding = train_dataset[N]\n",
    "# for k,v in encoding.items():\n",
    "#   print(k, v.shape)\n",
    "\n",
    "encoding = train_dataset[N]\n",
    "print(encoding[0].shape, encoding[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(10, 10), dpi=80)\n",
    "\n",
    "# plt.imshow(encoding['pixel_values'].permute(1,2,0))\n",
    "plt.imshow(encoding[0].permute(1,2,0), cmap=\"gray\")\n",
    "# plt.imshow(encoding[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_df['file_name'][N]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = encoding['labels']\n",
    "labels = encoding[1]\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    batch_dict = {}\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        if src_sample == None:\n",
    "            continue\n",
    "        if tgt_sample == None:\n",
    "            continue\n",
    "        src_batch.append(src_sample) #ADDEDLINE\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "\n",
    "    src_batch = torch.stack(src_batch)\n",
    "\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=-100)\n",
    "\n",
    "    batch_dict['pixel_values'] = src_batch\n",
    "    batch_dict['labels'] = tgt_batch\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "# books_dataloader = DataLoader(books, batch_size=16, collate_fn=collate_fn)\n",
    "# magazines_dataloader = DataLoader(magazines, batch_size=16, collate_fn=collate_fn)\n",
    "# novels_dataloader = DataLoader(novels, batch_size=16, collate_fn=collate_fn)\n",
    "# poetry_dataloader = DataLoader(poetry, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(train_dataloader):\n",
    "    print(x['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import VisionEncoderDecoderModel, RobertaConfig, RobertaForCausalLM, ViTModel\n",
    "# import torch\n",
    "from transformers import EncoderDecoderModel\n",
    "from models import *\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "conv, transformer = model_conv_transformer(inchannel=256, vocab_size=tokenizer.vocab_size)\n",
    "\n",
    "# transformer = EncoderDecoderModel.from_pretrained(\"conv_transformer_weights/icdar/\")\n",
    "\n",
    "\n",
    "# conv.load_state_dict(torch.load(\"conv_transformer_weights/icdar/conv.pt\"))\n",
    "\n",
    "conv.to(device)\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "transformer.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "transformer.config.vocab_size = transformer.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "transformer.config.eos_token_id = tokenizer.eos_token_id\n",
    "transformer.config.max_length = 256\n",
    "# transformer.config.max_new_tokens = 256 \n",
    "transformer.config.early_stopping = False\n",
    "transformer.config.no_repeat_ngram_size = 0\n",
    "transformer.config.length_penalty = 1\n",
    "transformer.config.num_beams = 4\n",
    "transformer.config.temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(enc, dec, data_loader, optimizer, ctc_loss):\n",
    "  enc.train()\n",
    "  dec.train()\n",
    "  train_loss = 0.0\n",
    "  train_loss_cross = 0.0\n",
    "  train_loss_ctc = 0.0\n",
    "  correct_train = 0.0\n",
    "  total_train = 0.0\n",
    "  train_acc = 0\n",
    "  for batch in tqdm(data_loader):\n",
    "    for k,v in batch.items():\n",
    "      batch[k] = v.to(device)\n",
    "\n",
    "    pixel_values = batch['pixel_values']\n",
    "    labels = batch['labels']\n",
    "\n",
    "\n",
    "    # pixel_values = transform(pixel_values)\n",
    "    emb = enc(pixel_values)\n",
    "    \n",
    "\n",
    "    # pixel_values = conv(pixel_values)\n",
    "    # loss_ctc = ctc_loss(pred_ctc, labels, input_lengths, output_lengths)\n",
    "\n",
    "    # logits, loss_cross_entropy = dec(tgt=labels[:, :-1], memory=emb, calc_loss=True, labels = labels[:,1:], tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "    outputs = transformer(inputs_embeds=emb, labels=labels)\n",
    "    logits = outputs.logits\n",
    "    loss = outputs.loss\n",
    "    # loss = outputs.loss\n",
    "\n",
    "    # loss = (loss_cross_entropy+loss_ctc)/2\n",
    "    # loss = loss_ctc\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    # train_loss_ctc += loss_ctc.item()\n",
    "    # train_loss_cross += loss_cross_entropy.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "      preds = torch.argmax(logits, axis=-1)\n",
    "      mask = torch.ones_like(labels).to(device)\n",
    "      mask[labels==-100] = 0\n",
    "      correct_train += ((preds == labels)*mask).sum()\n",
    "      total_train += mask.sum()\n",
    "    \n",
    "\n",
    "  \n",
    "  train_loss = train_loss/len(train_dataloader)\n",
    "  train_loss_cross = train_loss_cross/len(train_dataloader)\n",
    "  train_loss_ctc = train_loss_ctc/len(train_dataloader)\n",
    "  train_acc = (correct_train/total_train).to('cpu').item()\n",
    "\n",
    "\n",
    "  return train_loss, train_loss_cross, train_loss_ctc, train_acc\n",
    "\n",
    "\n",
    "def eval(enc, dec, data_loader, ctc_loss):\n",
    "  eval_loss = 0.0\n",
    "  eval_loss_cross = 0.0\n",
    "  eval_loss_ctc = 0.0\n",
    "  correct_eval = 0.0\n",
    "  total_eval = 0.0\n",
    "  eval_acc=0\n",
    "  enc.eval()\n",
    "  dec.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      pixel_values = batch['pixel_values']\n",
    "      labels = batch['labels']\n",
    "\n",
    "      # pixel_values = transform(pixel_values)\n",
    "      emb = enc(pixel_values)\n",
    "      # print(emb.shape)\n",
    "\n",
    "      # pixel_values = conv(pixel_values)\n",
    "      # loss_ctc = ctc_loss(pred_ctc, labels, input_lengths, output_lengths)\n",
    "      # print(tgt_mask.shape)\n",
    "      # print(tgt_key_padding_mask.shape)\n",
    "      # print(labels[:, 1:].shape)\n",
    "      # logits, loss_cross_entropy = dec(tgt=labels[:, :-1], memory=emb, calc_loss=True, labels = labels[:,1:], tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "      # print(a)\n",
    "      outputs = transformer(inputs_embeds=emb, labels=labels)\n",
    "      logits = outputs.logits\n",
    "      loss = outputs.loss\n",
    "      # loss = outputs.loss\n",
    "\n",
    "      # loss = (loss_cross_entropy+loss_ctc)/2\n",
    "      # loss = loss_ctc\n",
    "      eval_loss += loss.item()\n",
    "      # eval_loss_cross += loss_cross_entropy.item()\n",
    "      # eval_loss_ctc += loss_ctc.item()\n",
    "\n",
    "      preds = torch.argmax(logits, axis=-1)\n",
    "      mask = torch.ones_like(labels).to(device)\n",
    "      mask[labels==-100] = 0\n",
    "      correct_eval += ((preds == labels)*mask).sum()\n",
    "      total_eval += mask.sum()\n",
    "\n",
    "\n",
    "    eval_loss = eval_loss/len(eval_dataloader)\n",
    "    eval_loss_cross = eval_loss_cross/len(eval_dataloader)\n",
    "    eval_loss_ctc = eval_loss_ctc/len(eval_dataloader)\n",
    "    eval_acc = (correct_eval/total_eval).to('cpu').item()\n",
    "\n",
    "  return eval_loss, eval_loss_cross, eval_loss_ctc, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CTCLoss(blank=tokenizer.mask_token_id, zero_infinity=True)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "params = list(conv.parameters()) + list(transformer.parameters())\n",
    "# params = list(transformer.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(params, lr=0.0003)\n",
    "# optimizer = torch.optim.AdamW(params, lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.0003, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_eval_steps = num_epochs * len(eval_dataloader)\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_eval_loss, _, _, prev_eval_acc = eval(conv, transformer,\n",
    "                                           eval_dataloader, \n",
    "                                           loss_fn)\n",
    "print(prev_eval_loss, prev_eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "  if count > 1:\n",
    "    break\n",
    "\n",
    "  train_loss, train_loss_cross, train_loss_ctc, train_acc = train(conv, transformer,\n",
    "                                                                  train_dataloader, \n",
    "                                                                  optimizer, loss_fn)\n",
    "  print(f\"Train Loss after epoch {epoch}:\", train_loss)\n",
    "  print(f\"Train Loss_CROSS after epoch {epoch}:\", train_loss_cross)\n",
    "  print(f\"Train Loss_CTC after epoch {epoch}:\", train_loss_ctc)\n",
    "  print(f\"Train ACC after epoch {epoch}:\", train_acc*100)\n",
    "  \n",
    "  eval_loss, eval_loss_cross, eval_loss_ctc, eval_acc = eval(conv, transformer,\n",
    "                                                              eval_dataloader, \n",
    "                                                              loss_fn)\n",
    "  print(f\"VAL P Loss after epoch {epoch}:\", eval_loss)\n",
    "  # print(f\"VAL P Loss_CROSS after epoch {epoch}:\", eval_loss_cross)\n",
    "  # print(f\"VAL P Loss_CTC after epoch {epoch}:\", eval_loss_ctc)\n",
    "  print(f\"VAL P ACC after epoch {epoch}:\", eval_acc*100)\n",
    "\n",
    "\n",
    "  # eval_loss, eval_loss_cross, eval_loss_ctc, eval_acc = eval(conv, transformer,\n",
    "  #                                                             test_dataloader, \n",
    "  #                                                             loss_fn)\n",
    "  # print(f\"VAL H Loss after epoch {epoch}:\", eval_loss)\n",
    "  # # print(f\"VAL H Loss_CROSS after epoch {epoch}:\", eval_loss_cross)\n",
    "  # # print(f\"VAL H Loss_CTC after epoch {epoch}:\", eval_loss_ctc)\n",
    "  # print(f\"VAL H ACC after epoch {epoch}:\", eval_acc*100)\n",
    "\n",
    "\n",
    "  # if prev_eval_acc < eval_acc:\n",
    "  #   prev_eval_acc = eval_acc\n",
    "  #   # transformer.save_pretrained(\"./weights/convnext/\")\n",
    "  #   transformer.save_pretrained(\"./conv_transformer_weights/icdar\")\n",
    "  #   torch.save(conv.state_dict(), \"./conv_transformer_weights/icdar/conv.pt\")\n",
    "  #   print(\"WEIGHTS SAVED\")\n",
    "  \n",
    "  # else:\n",
    "  #   count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import cer\n",
    "from evaluate import load\n",
    "cer = load(\"cer\")\n",
    "\n",
    "\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "\n",
    "\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    print(pred_str[0])\n",
    "    print(label_str[0])\n",
    "\n",
    "    # print(label_str)\n",
    "\n",
    "    combine = [(x, y) for x, y in zip(pred_str, label_str) if x]\n",
    "\n",
    "    pred_str = [x for x, y in combine]\n",
    "    label_str = [y for x, y in combine]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # print(pred_str[0])\n",
    "    # print(label_str[0])\n",
    "\n",
    "    # if not pred_str:\n",
    "    #     return 0\n",
    "\n",
    "\n",
    "    # error = cer(pred_str, label_str)\n",
    "    cer_score = cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "\n",
    "    return cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# conv.to(device)\n",
    "# transformer.to(device)\n",
    "conv.eval()\n",
    "transformer.eval()\n",
    "test_cer = 0.0\n",
    "data_loader = eval_dataloader\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(data_loader):\n",
    "    # run batch generation\n",
    "    outputs = conv(batch[\"pixel_values\"].to(device))\n",
    "    outputs = transformer.generate(inputs_embeds=outputs)\n",
    "    # compute metrics|\n",
    "    error = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "    test_cer += error \n",
    "\n",
    "print(\"Validation CER:\", test_cer / len(data_loader)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "de2ab51c1de51922940535d3439132e1ddcc6db379bda0e9c111dda4b886f793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
